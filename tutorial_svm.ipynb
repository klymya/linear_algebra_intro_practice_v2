{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66767b42",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM) & Kernel Trick - From Scratch (with SciPy Optimizer)\n",
    "\n",
    "This unified tutorial is inspired by:\n",
    "- Kaggle: *SVM with Kernel Trick from Scratch*\n",
    "- GeeksforGeeks: *Implementing SVM and Kernel SVM with scikit-learn*\n",
    "- MachineLearningMastery: *Kernel Methods in Python*\n",
    "- QuarkML blog: *SVM kernels (polynomial, RBF)*\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Implement the **dual formulation** of a soft-margin SVM using NumPy.\n",
    "2. Use **SciPy's optimizer** to solve the dual problem (instead of writing SMO ourselves).\n",
    "3. Implement the **kernel trick** via linear and RBF kernels.\n",
    "4. Compare accuracy and decision boundaries of **linear vs RBF kernels** on `make_circles`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0925e81",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this tutorial, you should be able to:\n",
    "\n",
    "- Write down the soft-margin SVM **dual objective** and constraints.\n",
    "- Implement **kernel functions** (linear, RBF) and Gram matrices in NumPy.\n",
    "- Use **SciPy** to solve the constrained quadratic optimization problem for SVM.\n",
    "- Visualize and compare linear vs RBF SVMs on a non-linearly separable dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT THIS CELL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e9c77",
   "metadata": {},
   "source": [
    "## 1) Toy dataset: non-linear circles\n",
    "\n",
    "We use `make_circles` so that:\n",
    "\n",
    "- linear SVM fails to separate the classes well,\n",
    "- RBF SVM succeeds thanks to the kernel trick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=800, factor=0.5, noise=0.08, random_state=0)\n",
    "y = np.where(y == 1, 1, -1)  # convert labels to {-1, +1}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=10)\n",
    "plt.title(\"make_circles: train split\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c0bd8",
   "metadata": {},
   "source": [
    "## 2) Kernels\n",
    "\n",
    "Implement the **linear** and **RBF** kernels and a helper to compute the Gram matrix.\n",
    "\n",
    "We will use:\n",
    "\n",
    "- Linear:  \\(K(x,z) = x^\\top z\\)\n",
    "- RBF:     \\(K(x,z) = \\exp(-\\gamma \\|x - z\\|^2)\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979663f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(X1, X2):\n",
    "    \"\"\"Return linear kernel Gram matrix K_ij = <x_i, z_j>.\"\"\"\n",
    "    # TODO: implement linear kernel using matrix multiplication\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def rbf_kernel(X1, X2, gamma: float):\n",
    "    \"\"\"Return RBF kernel Gram matrix K_ij = exp(-gamma ||x_i - z_j||^2).\"\"\"\n",
    "    # Hint: use ||x - z||^2 = ||x||^2 + ||z||^2 - 2 xÂ·z\n",
    "    # TODO: implement RBF kernel in a fully vectorized way\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def compute_gram(X1, X2, kernel: str = \"linear\", gamma: float = 1.0):\n",
    "    \"\"\"Compute Gram matrix for the given kernel type.\"\"\"\n",
    "    # TODO: call the correct kernel based on the 'kernel' string\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b2f27",
   "metadata": {},
   "source": [
    "## 3) Dual SVM with SciPy optimizer\n",
    "\n",
    "We consider the soft-margin SVM dual:\n",
    "\n",
    "\\begin{align}\n",
    "\\max_{\\alpha} \\quad & \n",
    "  \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) \\\\\n",
    "\\text{s.t.} \\quad & 0 \\le \\alpha_i \\le C, \\quad \\forall i \\\\\n",
    "                  & \\sum_{i=1}^N \\alpha_i y_i = 0\n",
    "\\end{align}\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Precompute the **Gram matrix** \\\\(K\\\\).\n",
    "2. Precompute \\\\(Q_{ij} = y_i y_j K_{ij}\\\\).\n",
    "3. Define the **objective** to *minimize* (negative of the dual).\n",
    "4. Use `scipy.optimize.minimize` with:\n",
    "   - bounds `0 <= alpha_i <= C`,\n",
    "   - equality constraint `sum(alpha_i * y_i) = 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58044b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMFromScratch:\n",
    "    def __init__(self, C=1.0, kernel=\"linear\", gamma=1.0, random_state=0):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.alphas = None\n",
    "        self.b = 0.0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit SVM by solving the dual with SciPy's optimizer.\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y.astype(float)\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # Gram matrix\n",
    "        K = compute_gram(X, X, kernel=self.kernel, gamma=self.gamma)\n",
    "        # Q_ij = y_i y_j K_ij\n",
    "        Q = (self.y[:, None] * self.y[None, :]) * K\n",
    "\n",
    "        def objective(alpha):\n",
    "            # 0.5 * alpha^T Q alpha - 1^T alpha  (to MINIMIZE)\n",
    "            alpha = alpha.reshape(-1, 1)\n",
    "            return 0.5 * float(alpha.T @ Q @ alpha) - float(alpha.sum())\n",
    "\n",
    "        def objective_grad(alpha):\n",
    "            # gradient: Q alpha - 1\n",
    "            return (Q @ alpha) - np.ones_like(alpha)\n",
    "\n",
    "        # Equality constraint: sum_i alpha_i y_i = 0\n",
    "        constraints = ({\n",
    "            \"type\": \"eq\",\n",
    "            \"fun\": lambda a: np.dot(a, self.y),\n",
    "            \"jac\": lambda a: self.y\n",
    "        },)\n",
    "\n",
    "        bounds = [(0, self.C) for _ in range(N)]\n",
    "        alpha0 = np.zeros(N)\n",
    "\n",
    "        res = minimize(\n",
    "            fun=objective,\n",
    "            x0=alpha0,\n",
    "            jac=objective_grad,\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            method=\"SLSQP\",\n",
    "            options={\"maxiter\": 500, \"ftol\": 1e-6, \"disp\": False},\n",
    "        )\n",
    "\n",
    "        self.alphas = res.x\n",
    "\n",
    "        # Compute bias term b using support vectors with 0 < alpha_i < C\n",
    "        sv_mask = (self.alphas > 1e-6) & (self.alphas < self.C - 1e-6)\n",
    "        if not np.any(sv_mask):\n",
    "            # Fallback: just use any non-zero alpha\n",
    "            sv_mask = self.alphas > 1e-6\n",
    "\n",
    "        idx = np.where(sv_mask)[0]\n",
    "        if idx.size == 0:\n",
    "            self.b = 0.0\n",
    "        else:\n",
    "            # b = y_i - sum_j alpha_j y_j K_ij, averaged over SVs\n",
    "            b_vals = []\n",
    "            for i in idx:\n",
    "                s = np.sum(self.alphas * self.y * K[i])\n",
    "                b_vals.append(self.y[i] - s)\n",
    "            self.b = float(np.mean(b_vals))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X_new):\n",
    "        \"\"\"Compute f(x) = sum_i alpha_i y_i K(x_i, x) + b.\"\"\"\n",
    "        # TODO: use compute_gram with training data self.X and weights alpha_i * y_i\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        \"\"\"Return predicted labels in {-1, +1}.\"\"\"\n",
    "        # TODO: sign(decision_function)\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a23404",
   "metadata": {},
   "source": [
    "## 4) Train & compare: Linear vs RBF on `make_circles`\n",
    "\n",
    "Now we:\n",
    "\n",
    "1. Train **linear SVM** and evaluate on test set.\n",
    "2. Train **RBF SVM** and evaluate on test set.\n",
    "3. Plot decision boundaries for both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbcf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision boundary\"):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 200),\n",
    "        np.linspace(y_min, y_max, 200)\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, levels=[-1, 0, 1])\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=10)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b336888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train and evaluate linear SVM\n",
    "# svm_lin = SVMFromScratch(C=1.0, kernel=\"linear\").fit(X_train, y_train)\n",
    "# y_pred_lin = svm_lin.predict(X_test)\n",
    "# acc_lin = accuracy_score(y_test, y_pred_lin)\n",
    "# print(f\"Linear kernel accuracy: {acc_lin:.3f}\")\n",
    "# plot_decision_boundary(svm_lin, X_train, y_train, title=\"Linear kernel (train)\")\n",
    "\n",
    "raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc10062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train and evaluate RBF SVM (e.g., gamma=2.0)\n",
    "# svm_rbf = SVMFromScratch(C=1.0, kernel=\"rbf\", gamma=2.0).fit(X_train, y_train)\n",
    "# y_pred_rbf = svm_rbf.predict(X_test)\n",
    "# acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "# print(f\"RBF kernel accuracy: {acc_rbf:.3f}\")\n",
    "# plot_decision_boundary(svm_rbf, X_train, y_train, title=\"RBF kernel (train)\")\n",
    "\n",
    "raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c767c",
   "metadata": {},
   "source": [
    "## 5) Discussion\n",
    "\n",
    "In a short markdown cell (your own words):\n",
    "\n",
    "- How do linear vs RBF accuracies differ on `make_circles`? Why?\n",
    "- How does the **gamma** parameter of the RBF kernel affect the decision boundary?\n",
    "- What are the pros/cons of solving SVM via a generic optimizer (SciPy) vs specialized SMO implementations (like in scikit-learn)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
