{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07daf8b1",
   "metadata": {},
   "source": [
    "# PCA for Face Recognition (Eigenfaces) â€” From Scratch\n",
    "\n",
    "This notebook is a unified tutorial inspired by:\n",
    "\n",
    "- Mathematics for Machine Learning PCA tutorial & solution (mml-book.github.io)\n",
    "- GeeksforGeeks: *ML | Face Recognition Using PCA Implementation*\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Implement PCA **from scratch** on simple data.\n",
    "2. Extend this implementation to **Eigenfaces-based face recognition** using a real face dataset.\n",
    "3. Compare:\n",
    "   - SVM classifier on **original** high-dimensional face data,\n",
    "   - SVM classifier on **PCA-reduced** data preserving **90%** of variance.\n",
    "\n",
    "> Work in order, fill in all `TODO` sections, and run cells as you go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a091400",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "After completing this tutorial, you should be able to:\n",
    "\n",
    "- Explain the geometric idea behind PCA.\n",
    "- Implement PCA from scratch (centering, covariance, eigendecomposition, projection, reconstruction).\n",
    "- Apply PCA to high-dimensional face images (Eigenfaces).\n",
    "- Use PCA as a preprocessing step for classification.\n",
    "- Compare SVM performance:\n",
    "  - in full pixel space,\n",
    "  - in PCA space with 90% variance preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238850e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT THIS CELL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868e35d",
   "metadata": {},
   "source": [
    "## 1. PCA on a Simple 2D Dataset\n",
    "\n",
    "We start with a 2D toy dataset to build intuition:\n",
    "\n",
    "1. Center the data.\n",
    "2. Compute the covariance matrix.\n",
    "3. Compute eigenvalues and eigenvectors.\n",
    "4. Sort them by explained variance.\n",
    "5. Project data to 1D and reconstruct.\n",
    "\n",
    "You will implement helper functions; they will be reused later for faces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple correlated 2D dataset (example)\n",
    "rng = np.random.default_rng(0)\n",
    "N = 300\n",
    "x1 = rng.normal(loc=0.0, scale=2.0, size=N)\n",
    "x2 = 0.5 * x1 + rng.normal(loc=0.0, scale=0.5, size=N)\n",
    "X_2d = np.vstack([x1, x2]).T  # shape (N, 2)\n",
    "\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.4)\n",
    "plt.title(\"Toy 2D dataset\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54cf4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(X: np.ndarray):\n",
    "    \"\"\"Center data matrix X by subtracting the mean of each feature.\n",
    "\n",
    "    Args:\n",
    "        X: array of shape (n_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "        X_centered: centered data\n",
    "        mean: feature-wise mean, shape (n_features,)\n",
    "    \"\"\"\n",
    "    # TODO: implement centering\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312158e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_matrix(X_centered: np.ndarray):\n",
    "    \"\"\"Compute sample covariance matrix for centered data.\n",
    "\n",
    "    Args:\n",
    "        X_centered: array of shape (n_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "        Sigma: covariance matrix of shape (n_features, n_features)\n",
    "    \"\"\"\n",
    "    # TODO: implement covariance matrix\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_fit(X: np.ndarray):\n",
    "    \"\"\"Fit PCA using eigendecomposition of the covariance matrix.\n",
    "\n",
    "    Args:\n",
    "        X: array of shape (n_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "        mean: feature-wise mean, shape (n_features,)\n",
    "        components: matrix of principal directions (eigenvectors),\n",
    "                    shape (n_components, n_features)\n",
    "        eigenvalues: eigenvalues sorted in descending order, shape (n_components,)\n",
    "    \"\"\"\n",
    "    # Hints:\n",
    "    # 1. Center data using center_data.\n",
    "    # 2. Compute covariance matrix.\n",
    "    # 3. Compute eigenvalues and eigenvectors with np.linalg.eigh.\n",
    "    # 4. Sort by eigenvalue (descending).\n",
    "    # 5. Return all components (you can slice later).\n",
    "    # TODO: implement PCA fit\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(X: np.ndarray, mean: np.ndarray, components: np.ndarray, n_components: int):\n",
    "    \"\"\"Project X onto first n_components principal components.\n",
    "\n",
    "    Args:\n",
    "        X: array (n_samples, n_features)\n",
    "        mean: array (n_features,)\n",
    "        components: array (n_components_total, n_features)\n",
    "        n_components: number of components to keep\n",
    "\n",
    "    Returns:\n",
    "        Z: projected data, shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    # TODO: implement transform\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def pca_inverse_transform(Z: np.ndarray, mean: np.ndarray, components: np.ndarray, n_components: int):\n",
    "    \"\"\"Reconstruct data from PCA representation.\n",
    "\n",
    "    Args:\n",
    "        Z: projected data (n_samples, n_components)\n",
    "        mean: array (n_features,)\n",
    "        components: array (n_components_total, n_features)\n",
    "        n_components: number of components used for projection\n",
    "\n",
    "    Returns:\n",
    "        X_recon: reconstructed data, shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    # TODO: implement inverse transform\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: run PCA on the toy dataset (after implementing the functions)\n",
    "mean_2d, comps_2d, vals_2d = pca_fit(X_2d)\n",
    "print(\"Mean:\", mean_2d)\n",
    "print(\"Eigenvalues:\", vals_2d)\n",
    "\n",
    "Z_1d = pca_transform(X_2d, mean_2d, comps_2d, n_components=1)\n",
    "X_2d_recon = pca_inverse_transform(Z_1d, mean_2d, comps_2d, n_components=1)\n",
    "\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.3, label=\"Original\")\n",
    "plt.scatter(X_2d_recon[:, 0], X_2d_recon[:, 1], alpha=0.7, label=\"Reconstruction (1 PC)\")\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.title(\"PCA reconstruction with 1 principal component\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4055f17",
   "metadata": {},
   "source": [
    "## 2. PCA for Face Images (Eigenfaces)\n",
    "\n",
    "Now we move to a real high-dimensional dataset.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load a labeled face dataset.\n",
    "2. Flatten each face image into a vector.\n",
    "3. Use our **from-scratch PCA** to compute Eigenfaces.\n",
    "4. Visualize leading Eigenfaces.\n",
    "5. Use PCA features together with SVM for face recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LFW people dataset (downloads if not already present)\n",
    "lfw = fetch_lfw_people(min_faces_per_person=50, resize=0.4, color=False)\n",
    "X_faces = lfw.data  # shape (n_samples, n_features)\n",
    "y_faces = lfw.target\n",
    "target_names = lfw.target_names\n",
    "h, w = lfw.images.shape[1:]\n",
    "\n",
    "print(\"Faces shape:\", X_faces.shape)\n",
    "print(\"Image size: {}x{}\".format(h, w))\n",
    "print(\"Number of classes:\", len(target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few example faces\n",
    "def plot_faces(images, titles=None, h=0, w=0, n_row=3, n_col=6):\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    for i in range(n_row * n_col):\n",
    "        if i >= images.shape[0]:\n",
    "            break\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap='gray')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        if titles is not None:\n",
    "            plt.title(titles[i], fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_faces(lfw.images, n_row=3, n_col=6, h=h, w=w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit PCA (from scratch) to the face dataset for Eigenfaces visualization\n",
    "# 1. Use your pca_fit on X_faces\n",
    "# 2. Store mean_faces, components_faces, eigenvalues_faces\n",
    "# TODO:\n",
    "raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot explained variance and cumulative explained variance vs number of components\n",
    "# After you have eigenvalues_faces:\n",
    "# 1. Compute explained_var_ratio = eigenvalues_faces / eigenvalues_faces.sum()\n",
    "# 2. Compute cumulative_var = np.cumsum(explained_var_ratio)\n",
    "# 3. Plot:\n",
    "#    - explained_var_ratio\n",
    "#    - cumulative_var\n",
    "# Use the x-axis as component index (starting from 1).\n",
    "raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After implementing PCA above, visualize the leading Eigenfaces\n",
    "\n",
    "def plot_eigenfaces(components, h, w, n_row=3, n_col=6):\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    for i in range(n_row * n_col):\n",
    "        if i >= components.shape[0]:\n",
    "            break\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(components[i].reshape((h, w)), cmap='gray')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.title(f\"PC {i+1}\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment when components_faces is defined\n",
    "# plot_eigenfaces(components_faces[:36], h, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d7f86",
   "metadata": {},
   "source": [
    "## 3. Face Recognition with SVM: Full Space vs PCA Space\n",
    "\n",
    "We now build two SVM-based classifiers:\n",
    "\n",
    "1. **SVM (full space)**: Train SVM directly on original pixel vectors.\n",
    "2. **SVM (PCA space)**: \n",
    "   - Fit PCA **on training data only** using your implementation.\n",
    "   - Choose the smallest number of components that preserve at least **90%** of total variance.\n",
    "   - Train SVM on PCA features (train and test projected to this space).\n",
    "\n",
    "Then we compare accuracies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_faces, y_faces, test_size=0.25, random_state=42, stratify=y_faces\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train an SVM classifier on FULL data (no PCA) and evaluate accuracy.\n",
    "\n",
    "# Suggested baseline:\n",
    "# svm_full = SVC(kernel=\"linear\", class_weight=\"balanced\", random_state=42)\n",
    "# svm_full.fit(X_train, y_train)\n",
    "# y_pred_full = svm_full.predict(X_test)\n",
    "# acc_full = accuracy_score(y_test, y_pred_full)\n",
    "# print(f\"SVM (full space) accuracy: {acc_full:.3f}\")\n",
    "\n",
    "raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ad35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PCA on training data + SVM in PCA space with 90% variance preserved.\n",
    "\n",
    "# Steps:\n",
    "# 1. Fit PCA on X_train using pca_fit -> mean_train, comps_train, vals_train.\n",
    "# 2. Compute explained_var_ratio from vals_train.\n",
    "# 3. Find minimal n_components_90 such that cumulative variance >= 0.90.\n",
    "# 4. Project X_train and X_test into this PCA space using pca_transform.\n",
    "# 5. Train another SVM on Z_train_90 and evaluate on Z_test_90.\n",
    "# 6. Print:\n",
    "#    - n_components_90\n",
    "#    - SVM (PCA 90%) accuracy.\n",
    "\n",
    "raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd38972",
   "metadata": {},
   "source": [
    "## 4. Discussion\n",
    "\n",
    "Answer in your own words (no code required here):\n",
    "\n",
    "- How do the two accuracies compare?\n",
    "- What are the trade-offs between:\n",
    "  - SVM in full dimension vs\n",
    "  - SVM in PCA-reduced space with 90% variance?\n",
    "- How does PCA help with:\n",
    "  - Dimensionality reduction,\n",
    "  - Training time,\n",
    "  - Potential overfitting?\n",
    "\n",
    "You can also:\n",
    "- Try different variance thresholds (e.g. 95%).\n",
    "- Try different SVM kernels.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-ds-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
